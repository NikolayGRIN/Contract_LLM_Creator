Модели

Проект Contract LLM Creator (Offline) использует только локальные языковые модели
в формате GGUF через библиотеку `llama-cpp-python`.
Все этапы инференса выполняются полностью оффлайн.

Используемая модель (референсная)

- Семейство модели: **Qwen2.5 Instruct**
- Формат / квантизация: **GGUF (q4_k_m)**
- Пример файлов модели:
  - `qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf`
  - `qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf` (если модель разбита на части)

Размещение модели: Contract LLM Creator/models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf
qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf

Путь к модели задаётся в файле `src/config.py`

Пример конфигурации:

```python
LOCAL_GGUF_MODEL_PATH = "models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"

N_CTX = 4096
N_THREADS = 8
TEMPERATURE = 0.3
TOP_P = 0.92
MAX_TOKENS = 1600

Оффлайн-инференс

После установки зависимостей и размещения модели:
не используются облачные API,
отсутствуют внешние LLM-сервисы,
генерация выполняется локально через llama-cpp-python.

Это обеспечивает:
конфиденциальность данных,
воспроизводимость экспериментов,
применимость в корпоративной и юридической среде.

В Google Colab модель может храниться в Google Drive с указанием абсолютного пути.

